---
title: "Optimal control rate estimation for a case-control study"
author: "Wouter van Amsterdam"
date: 2017-10-12
output: html_document
---

<!-- The file analysis/chunks.R contains chunks that define default settings
shared across the workflowr files. -->
```{r read-chunk, include=FALSE, cache=FALSE}
knitr::read_chunk("chunks.R")
```

<!-- Update knitr chunk options -->
```{r knitr-opts-chunk, include=FALSE}
```

<!-- Insert the date the file was last updated -->
```{r last-updated, echo=FALSE, results='asis'}
```

<!-- Insert the code version (Git commit SHA1) if Git repository exists and R
 package git2r is installed -->
```{r code-version, echo=FALSE, results='asis'}
```

<!-- Add your analysis here -->
# Question
Given an expected odds-ratio $OR = 2$, and an exposure-prevalence in the source-population of $p_{1} = 0.05$, what is the optimal ratio of cases and controls? 

The goal is to minimize the variance of the log odds-ratio, narrowing the 95% confidence interval with respect to the cases:controls ratio, given an expected OR and exposure prevalance in the source-population.

$min(Var(ln(OR)))$

Where

$Var(ln(OR)) = \frac{1}{a} + \frac{1}{b} + \frac{1}{c} + \frac{1}{d}$

With {a-d} the entries of the standard 2x2 contingency table.

## Analytical solution
Whe can achieve this by writing each of the entries as a function of the $OR$, $p_{1}$ and 
$r_{1} = \frac{N_{Cases}}{N}$, and differentiating with respect to $r_{1}$.

The analytical solution for this problem is

$r_{1} = \frac{-2 \pm \sqrt{4 - 4*\alpha*(-1))}}{2\alpha}$

With

$\alpha = \frac{p_{1}(1-p_{1})}{p_{0}(1-p_{0})}-1$ and $p_{1} = \frac{Odds_{1}}{1+Odds_{1}} = \frac{OR*Odds_{0}}{1 + OR*Odds_{0}}$

Where $Odds_{0}$ is the odds for controls: $Odds_{0} = \frac{p_{0}}{1-p_{0}}$

For this particular instance:
```{r}
OR = 2
p0 = 0.05

# the math
O0 = p0/(1-p0)
O1 = OR * O0
p1 = O1/(1+O1)
a  = ((p1*(1-p1))/(p0*(1-p0))) - 1


r1 = c(
  (-2+sqrt(4 - 4*a*(-1)))/(2*a),
  (-2-sqrt(4 - 4*a*(-1)))/(2*a))
```

This yields 2 results for $r_{1}$: $\left\{`r round(r1[1], 2)`, `r round(r1[2], 2)`\right\}$, where 
$r_{1} = `r round(r1[1], 2)`$ is the only viable option.

Now the ratio of cases:controls

$\frac{N_{cases}}{N_{controls}} = \frac{r_{1}}{1-r_{1}} = `r round(r1[1]/(1-r1[1]), 2)`$

For a range of odds ratios and exposure rates, with plot
```{r, message=F}
require(data.table)
require(ggplot2)
require(magrittr)

OR = 1:10
p0_values = c(0.01, 0.05, 0.15, 0.25, 0.5, 0.75, 0.85, 0.95, 0.99)


control_array = matrix(nrow = length(OR), ncol = length(p0_values))
for (i in 1:length(p0_values)) {
  p0 = p0_values[i]
  
  # the math
  O0 = p0/(1-p0)
  O1 = OR * O0
  p1 = O1/(1+O1)
  a  = ((p1*(1-p1))/(p0*(1-p0))) + 1
  
  
  r1 = c(
    (-2+sqrt(4 - 4*a*(-1)))/(2*a),
    (-2-sqrt(4 - 4*a*(-1)))/(2*a))
  r1 = r1[r1>0] # pick only the values greater than zero
  ncontrols = (1-r1) / r1
  control_array[,i] = ncontrols
}

df <- as.data.table(control_array)
df[, OR:=OR]
df_melt <- melt(df, id.vars = "OR", 
                variable.name = "exposure_prevalence_source_population", value.name = "n_controls")
df_melt[, exposure_prevalence:=factor(exposure_prevalence_source_population, 
                                      levels = setdiff(colnames(df), "OR"),
                                      labels = p0_values)]

# plot(OR, ncontrols, 
#      xlim = c(0, max(OR)),
#      ylim = c(0, ceiling(max(ncontrols))),
#      ylab = "Number of controls per case", xlab = "Expected odds ratio", 
#      main = "Optimal number of controls in a case-control study", sub = "given an exposure prevalence = 0.05 in the source population")

df_melt %>%
  ggplot(aes(x = OR, y = n_controls, col = exposure_prevalence_source_population)) + 
  geom_line() + 
  labs(x = "Expected odds ratio", y = "Optimal number of controls",
       title = "Optimal number of controls in a case-control study")

```

## Computational minimum for variance of odds-ratio
Recall that 

$Var(ln(OR)) = \frac{1}{a} + \frac{1}{b} + \frac{1}{c} + \frac{1}{d}$

Where

$a = N*r1*p_{1}$ Number of cases with exposure; $p_{1}$ is calculated from $p_{0}$ and $OR$

$b = N*(1-r1)*p_{0}$ Number of controls with exposure

$c = N*r1*(1-p_{1})$ Number of cases without exposure

$d = N*(1-r1)*(1-p_{0})$ Number of controls without exposure

The variables $\left\{p_{0}, p_{1}\right\}$ are given by the case description. This equation should be minimized with respect to $r_{1}$. 

- note 1: The variance of the odds ratio is given as the sum of the inverses of the entries of the contingency table, therefore the exact position of which count goes to which cell is irrelevant to the current question
- note 2: The total sample size $N$ is a factor equally present in each of the contingency table counts, and thus will not be relevant to the minimization problem with respect to $r_{1}$. The absolute variance of the odds ratio will of course depend on the total sample size. 

```{r}
OR = 2
p0 = 0.05

# the math
O0 = p0/(1-p0)
O1 = OR * O0
p1 = O1/(1+O1)

n = 1000
r1_seq = seq(0.01, 0.99, by = 0.01)

a = n * r1_seq * p1
b = n * (1-r1_seq) * p0
c = n * r1_seq * (1-p1)
d = n * (1-r1_seq) * (1-p0)

var_ln_OR <- (1/a) + (1/b) + (1/c) + (1/d)

plot(r1_seq, var_ln_OR, type = "l", 
     xlab = "case rate (n_cases / n_total)", ylab = "Var(ln(OR))", 
     main = "Variance of log-odds ratio for different sampling scenarios", sub = "OR = 2, p0 = 0.05")

```

In this plot we see that for a wide range of case rates $r_{1}$, the variance in the odds-ratio will be similar. For the chosen sequence of case-rates, the variance was minimal for $r_{1} = `r r1_seq[which.min(var_ln_OR)]`$. However, this was determined by the initially entered case rates. Now to find the minimum variance with a computational approach.

```{r}
OR = 2
p0 = 0.05

# the math
O0 = p0/(1-p0)
O1 = OR * O0
p1 = O1/(1+O1)

n = 1000
r1_seq = seq(0.01, 0.99, by = 0.01)

a = n * r1_seq * p1
b = n * (1-r1_seq) * p0
c = n * r1_seq * (1-p1)
d = n * (1-r1_seq) * (1-p0)

var_ln_OR <- (1/a) + (1/b) + (1/c) + (1/d)
var_ln_OR_func <- function(x) {
  1/(x*p1) + 1/((1-x)*p0) + 1/(x*(1-p1)) + 1/((1-x)*(1-p0))
}

# library(ROI)
# 
# f_obj <- F_objective(F = var_ln_OR_func, n = 1)
# opt_problem <- OP(f_obj, 
#                   L_constraint(
#                     L = matrix(c(1,1), ncol = 1),
#                     dir = c(">", "<"),
#                     rhs = c(0, 1)
#                   ))
# 
# opt_problem <- OP(f_obj, 
#                   bounds = V_bound(li = 1, ui = 1, lb = 0, ub = 1)
#                   )
#   
# ROI_solve(opt_problem, start = 0.5)

optimize(f = var_ln_OR_func, interval = c(0,1))

```


## Compare with standard power calculations
```{r}
OR = 2
p0 = 0.05

# the math
O0 = p0/(1-p0)
O1 = OR * O0
p1 = O1/(1+O1)

n = 1000
r1_seq = seq(0.01, 0.99, by = 0.01)

# see if results match from SPSS provided by instructor
power.prop.test(n = c(600,600), p1 = p1, p2 = p0, sig.level = 0.05)

# find optimal power
powers <- sapply(r1_seq, function(r1) {
  max(power.prop.test(n = c(n*r1, n*(1-r1)), p1 = p1, p2 = p0, sig.level = 0.05)$power)
})


plot(r1_seq, powers, xlab = "case rate (n_cases / n_total)", ylab = "power", main = "Power for different sampling scenarios", sub = "OR = 2, p0 = 0.05")

```

Here, the optimal power is found at $r_{1} = `r r1_seq[which.min(powers)]`$



## Different power calculations by Frank Harrell's functions
Using the package Hmisc by Frank Harrell, different power calculations are done.
```{r, message = F}
library(Hmisc)
OR = 2
p0 = 0.05

# some math
O0 = p0/(1-p0)
O1 = OR * O0
p1 = O1/(1+O1)

ballocation(p1 = p1, p2 = p0, alpha = 0.5)

```

These results show that different ratios are optimal for different optimization goals. 

From the documentation of the Hmisc package:

> Given p1, p2, ballocation uses the method of Brittain and Schlesselman to compute the optimal fraction of observations to be placed in group 1 that either (1) minimize the variance of the difference in two proportions, (2) minimize the variance of the ratio of the two proportions, (3) minimize the variance of the log odds ratio, or (4) maximize the power of the 2-tailed test for differences. For (4) the total sample size must be given, or the fraction optimizing the power is not returned. The fraction for (3) is one minus the fraction for (1).

More explanation can be found here: [bpower](ftp://ftp.uni-bayreuth.de/pub/math/statlib/S/Harrell/help/Hmisc/html/bpower.html)

# Conclusions
1. The analytical solution and the computational solution for minimizing the $Var(ln(OR))$ are the same.
2. The optimal power calculation does not seem to equal the solution for the narrowest confidence interval of $ln(OR)$. Could this be related to the asymmetric confidence bounds for the OR? Power seeks to optimize the probability of finding a lower bound > 1 given a true $OR > 1$, while narrowing the confidence interval also cares about the upper limit.


## Session information

<!-- Insert the session information into the document -->
```{r session-info}
```
